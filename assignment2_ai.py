# -*- coding: utf-8 -*-
"""Assignment2 - AI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xdXIdt3ebeiRZunJcincDOmz4PS2VdBu
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/male_players (legacy).csv')

df.head()

start1 = 0
end1 = 5
start2 = 82

columns_to_drop = list(df.columns[start1:end1]) + \
                    ['dob', 'club_jersey_number','club_joined_date','club_contract_valid_until_year',
                    'nationality_id','nation_team_id','nation_jersey_number','real_face','release_clause_eur'] + \
                    list(df.columns[start2: ])



df.drop(columns=columns_to_drop,axis=1,inplace=True)
df

threshold = 0.1
columns_with_missing_values = df.columns[df.isnull().mean() > threshold]
df.drop(columns_with_missing_values,axis=1,inplace=True)

df

df.info()

numeric_data=df.select_dtypes(include=np.number)
non_numeric=df.select_dtypes(include = ['object'])

from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
imp = IterativeImputer(max_iter=10, random_state=0)
numeric_data = pd.DataFrame(np.round(imp.fit_transform(numeric_data)), columns=numeric_data.copy().columns)

df.drop(non_numeric,axis=1,inplace=True)
df

corr_matrix = df.corr()
pd.set_option('display.max_rows',None)
corr_matrix['overall'].sort_values()

limit = 0.4
correlation = corr_matrix[corr_matrix['overall']>limit].index
selected = df[correlation]
selected.info()

y = selected['overall']
x = selected.drop(['overall'], axis=1)

pd.Series(y).value_counts()

Xtrain,Xtest,Ytrain,Ytest=train_test_split(x,y,test_size=0.1,random_state=42,stratify=y)

from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression

dt=DecisionTreeClassifier(criterion='entropy')
knn=KNeighborsClassifier(n_neighbors=7)
#sv=SVC(probability=True)
#nb=GaussianNB()
#LR=LogisticRegression()

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, roc_auc_score, auc
import pickle as pkl

from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy='mean')
Xtrain = imputer.fit_transform(Xtrain)
Xtest = imputer.transform(Xtest)

for model in (dt, knn):
 model.fit(Xtrain, Ytrain)
 pkl.dump(model, open('/content/' + model.__class__.__name__ + '.pkl', 'wb'))
 y_pred = model.predict(Xtest)
 print(model.__class__.__name__, confusion_matrix(Ytest, y_pred), classification_report(Ytest, y_pred))

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(Xtrain, Ytrain)

predictions = rf.predict(Xtest)

mse = mean_squared_error(Ytest, predictions)
mae = mean_absolute_error(Ytest, predictions)
r2 = r2_score(Ytest, predictions)

print("Mean Squared Error:", mse)
print("Mean Absolute Error:", mae)
print("R-squared:", r2)

from sklearn.ensemble import GradientBoostingRegressor
gb = GradientBoostingRegressor(n_estimators=100, random_state=42)
gb.fit(Xtrain, Ytrain)
predictions= gb.predict(Xtest)
mae = mean_absolute_error(Ytest, predictions)
mse = mean_squared_error(Ytest, predictions)
r2 = r2_score(Ytest, predictions)

print("Mean Absolute Error:", mae)
print("Mean Squared Error:",mse)
print("R-squared:", r2)

from xgboost import XGBRegressor
xgb = XGBRegressor(n_estimators=100, random_state=42)
xgb.fit(Xtrain, Ytrain)
predictions= xgb.predict(Xtest)
mae = mean_absolute_error(Ytest, predictions)
mse = mean_squared_error(Ytest, predictions)
r2 = r2_score(Ytest, predictions)

print("Mean Absolute Error:", mae)
print("Mean Squared Error:",mse)
print("R-squared:", r2)

from sklearn.model_selection import GridSearchCV

parameter_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.1, 0.01, 0.001]
}

xgb = XGBRegressor()
grid_search = GridSearchCV(xgb, parameter_grid, cv=5)
grid_search.fit(Xtrain, Ytrain)

best_params = grid_search.best_params_
best_model = grid_search.best_estimator_
best_params

parameter_grid2 = {
    'n_estimators' : [50,100,200],
    'max_depth' : [3,5,7],
    'learning_rate' : [0.1,0.01,0.001]
}
gb = GradientBoostingRegressor()
grid_search2 = GridSearchCV(gb, parameter_grid2, cv=5)
grid_search2.fit(Xtrain, Ytrain)
best_params2 = grid_search2.best_params_
best_model2 = grid_search2.best_estimator_
best_params2

parameter_grid3 = {
    'n_estimators' : [50,100,200],
    'max_depth' : [3,5,7],
    'min_samples_split' : [2,5,10],

}
rf = RandomForestRegressor()
grid_search3 = GridSearchCV(rf, parameter_grid3, cv=5)
grid_search3.fit(Xtrain, Ytrain)
best_params3 = grid_search3.best_params_
best_model3 = grid_search3.best_estimator_
best_params3

def data_prep(data):
  start1 = 0
  end1 = 5
  start2 = 82

  columns_to_drop = list(data.columns[start1:end1]) + \
                    ['dob', 'club_jersey_number','club_joined_date','club_contract_valid_until_year',
                    'nationality_id','nation_team_id','nation_jersey_number','real_face','release_clause_eur'] + \
                    list(data.columns[start2: ])



  data.drop(columns=columns_to_drop,axis=1,inplace=True)

  threshold = 0.1
  columns_with_missing_values = data.columns[data.isnull().mean() > threshold]
  data.drop(columns_with_missing_values,axis=1,inplace=True)

  numeric_data=data.select_dtypes(include=np.number)
  non_numeric=data.select_dtypes(include = ['object'])

  from sklearn.experimental import enable_iterative_imputer
  from sklearn.impute import IterativeImputer
  imp = IterativeImputer(max_iter=10, random_state=0)
  numeric_data = pd.DataFrame(np.round(imp.fit_transform(numeric_data)), columns=numeric_data.copy().columns)
  data.drop(non_numeric,axis=1,inplace=True)

  corr_matrix = data.corr()
  pd.set_option('display.max_rows',None)
  corr_matrix['overall'].sort_values()

  limit = 0.4
  correlation = corr_matrix[corr_matrix['overall']>limit].index
  selected = data[correlation]

  yt = selected['overall']
  xt = selected.drop(['overall'], axis=1)
  return xt,yt

new_data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/players_22-1.csv')

new_data.head()

xt,yt = data_prep(new_data)

Xtrain,Xtest,Ytrain,Ytest=train_test_split(xt,yt,test_size=0.1,random_state=42,stratify=y)

best_model1.fit(Xtrain, Ytrain)
predictions= best_model1.predict(Xtest)
mae = mean_absolute_error(Ytest, predictions)
mse = mean_squared_error(Ytest, predictions)
r2 = r2_score(Ytest, predictions)

print("Mean Absolute Error:", mae)
print("Mean Squared Error:",mse)
print("R-squared:", r2)

best_model2.fit(Xtrain, Ytrain)
predictions= best_model2.predict(Xtest)
mae = mean_absolute_error(Ytest, predictions)
mse = mean_squared_error(Ytest, predictions)
r2 = r2_score(Ytest, predictions)

print("Mean Absolute Error:", mae)
print("Mean Squared Error:",mse)
print("R-squared:", r2)

best_model3.fit(Xtrain, Ytrain)
predictions = best_model3.predict(Xtest)
mae = mean_absolute_error(Ytest, predictions)
mse = mean_squared_error(Ytest, predictions)
r2 = r2_score(Ytest, predictions)

print("Mean Absolute Error:", mae)
print("Mean Squared Error:",mse)
print("R-squared:", r2)